{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R_9 Twitter Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 13:32:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"COMP90024_A2_EDA\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read In Original\n",
    "spark_json = spark.read.json('../data/raw/BigTwitterFile/twitter-huge.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Key Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important features to extract out of the original dataframe\n",
    "df_selected_columns = spark_json.select('doc._id', 'doc.data.created_at', 'doc.includes', \n",
    "                     'doc.data.lang', 'doc.data.text', 'doc.data.author_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter no geolocation and non english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----+--------------------+------------------+\n",
      "|                _id|          created_at|            includes|lang|                text|         author_id|\n",
      "+-------------------+--------------------+--------------------+----+--------------------+------------------+\n",
      "|1491734461951909890|2022-02-10T11:23:...|{\"places\":[{\"full...|  hi|@AshramGzb @Ashra...|858950980989140993|\n",
      "|1491734528779763719|2022-02-10T11:23:...|{\"places\":[{\"full...|  hi|@AshramGzb @naren...|858950980989140993|\n",
      "|1491567527322808321|2022-02-10T00:19:...|{\"places\":[{\"full...|  en|My life is hittin...|          45472006|\n",
      "|1491693811663515654|2022-02-10T08:41:...|{\"places\":[{\"full...|  en|@TobyRayEnglish @...|952342256823943168|\n",
      "|1491674087378219009|2022-02-10T07:23:...|{\"places\":[{\"full...|  en|@JadeArchaeobot @...|          25033901|\n",
      "+-------------------+--------------------+--------------------+----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns with no geolocation\n",
    "column_name = 'includes'\n",
    "df_drop_no_geo = df_selected_columns.filter(col(column_name).isNotNull())\n",
    "\n",
    "df_drop_no_geo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----+--------------------+---------+\n",
      "|                _id|          created_at|            includes|lang|                text|author_id|\n",
      "+-------------------+--------------------+--------------------+----+--------------------+---------+\n",
      "|1491567527322808321|2022-02-10T00:19:...|{\"places\":[{\"full...|  en|My life is hittin...| 45472006|\n",
      "+-------------------+--------------------+--------------------+----+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop non english coliumns\n",
    "column_name_2 = 'lang'\n",
    "string_value = 'en'\n",
    "df_drop_no_geo_eng = df_drop_no_geo.filter(col(column_name_2) == string_value)\n",
    "\n",
    "df_drop_no_geo_eng.show(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Tweets containing AI Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "words_df = pd.read_excel('../data/raw/Keywords/Voting Keyword.xlsx')\n",
    "\n",
    "words = list(words_df['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for political text\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import when, col, instr\n",
    "\n",
    "column_name = 'text'\n",
    "new_column_name = 'is_political'\n",
    "\n",
    "\n",
    "condition = reduce(lambda a, b: a | b, [instr(col(column_name), kw) > 0 for kw in words])\n",
    "\n",
    "df_drop_no_geo_eng_withPolCol = df_drop_no_geo_eng.withColumn(new_column_name, when(condition, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----+--------------------+-------------------+------------+----------+\n",
      "|                _id|          created_at|            includes|lang|                text|          author_id|is_political|      date|\n",
      "+-------------------+--------------------+--------------------+----+--------------------+-------------------+------------+----------+\n",
      "|1491567527322808321|2022-02-10T00:19:...|{\"places\":[{\"full...|  en|My life is hittin...|           45472006|           0|2022-02-10|\n",
      "|1491693811663515654|2022-02-10T08:41:...|{\"places\":[{\"full...|  en|@TobyRayEnglish @...| 952342256823943168|           0|2022-02-10|\n",
      "|1491674087378219009|2022-02-10T07:23:...|{\"places\":[{\"full...|  en|@JadeArchaeobot @...|           25033901|           0|2022-02-10|\n",
      "|1491721359587627008|2022-02-10T10:31:...|{\"places\":[{\"full...|  en|@JadeArchaeobot @...|         3103790508|           0|2022-02-10|\n",
      "|1491595343838220291|2022-02-10T02:10:...|{\"places\":[{\"full...|  en|@BehnamAkhavan @E...|1249497357944672257|           0|2022-02-10|\n",
      "+-------------------+--------------------+--------------------+----+--------------------+-------------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change to date\n",
    "from pyspark.sql.functions import substring, to_date\n",
    "\n",
    "df_drop_no_geo_eng_withPolCol_date = df_drop_no_geo_eng_withPolCol.withColumn(\"date\", to_date(substring(\"created_at\", 1, 10)))\n",
    "\n",
    "df_drop_no_geo_eng_withPolCol_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# extract bounding box coord\n",
    "df_drop_no_geo_eng_withPolCol_date_coord = df_drop_no_geo_eng_withPolCol_date.withColumn(\"coord\", regexp_extract(\"includes\", r\"(-?\\d+\\.\\d+,-?\\d+\\.\\d+,-?\\d+\\.\\d+,-?\\d+\\.\\d+)\", 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "# get the coordinate x and y for the bounding boxinto into separate columns\n",
    "df_drop_no_geo_eng_withPolCol_date_coord = df_drop_no_geo_eng_withPolCol_date_coord.withColumn(\"x1\", split(\"coord\", \",\").getItem(0).cast('float'))\n",
    "df_drop_no_geo_eng_withPolCol_date_coord = df_drop_no_geo_eng_withPolCol_date_coord.withColumn(\"y1\", split(\"coord\", \",\").getItem(1).cast('float'))\n",
    "df_drop_no_geo_eng_withPolCol_date_coord = df_drop_no_geo_eng_withPolCol_date_coord.withColumn(\"x2\", split(\"coord\", \",\").getItem(2).cast('float'))\n",
    "df_drop_no_geo_eng_withPolCol_date_coord = df_drop_no_geo_eng_withPolCol_date_coord.withColumn(\"y2\", split(\"coord\", \",\").getItem(3).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# get the centre coordinates for the bounding box\n",
    "df_drop_no_geo_eng_withPolCol_date_coord = df_drop_no_geo_eng_withPolCol_date_coord.withColumn(\"x_cent\", (df_drop_no_geo_eng_withPolCol_date_coord[\"x1\"] + df_drop_no_geo_eng_withPolCol_date_coord[\"x2\"]) / 2)\n",
    "df_drop_no_geo_eng_withPolCol_date_coord = df_drop_no_geo_eng_withPolCol_date_coord.withColumn(\"y_cent\", (df_drop_no_geo_eng_withPolCol_date_coord[\"y1\"] + df_drop_no_geo_eng_withPolCol_date_coord[\"y2\"]) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the remaining important cols and export\n",
    "df_drop_no_geo_eng_withPolCol_date_coord = df_drop_no_geo_eng_withPolCol_date_coord.select('_id', 'date', 'x_cent', 'y_cent', \n",
    "                     'text', 'author_id', 'is_political')\n",
    "df_drop_no_geo_eng_withPolCol_date_coord.write.format(\"parquet\").mode(\"overwrite\").save(\"../data/curated/Twitter/twitter.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>date</th>\n",
       "      <th>x_cent</th>\n",
       "      <th>y_cent</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>is_political</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1491567527322808321</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>150.520142</td>\n",
       "      <td>-23.339474</td>\n",
       "      <td>My life is hitting a big change soon. Keen to ...</td>\n",
       "      <td>45472006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1491693811663515654</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>153.369354</td>\n",
       "      <td>-27.954222</td>\n",
       "      <td>@TobyRayEnglish @MikeDel21893959 @aSinister @b...</td>\n",
       "      <td>952342256823943168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1491674087378219009</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>151.926880</td>\n",
       "      <td>-27.573589</td>\n",
       "      <td>@JadeArchaeobot @HarvardGSAS oh ewwww</td>\n",
       "      <td>25033901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1491721359587627008</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>115.928314</td>\n",
       "      <td>-32.150101</td>\n",
       "      <td>@JadeArchaeobot @HarvardGSAS I'm so sorry you ...</td>\n",
       "      <td>3103790508</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1491595343838220291</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>150.931976</td>\n",
       "      <td>-33.848244</td>\n",
       "      <td>@BehnamAkhavan @EngAustralia @Eng_IT_Sydney @S...</td>\n",
       "      <td>1249497357944672257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517272</th>\n",
       "      <td>1557517139002925056</td>\n",
       "      <td>2022-08-10</td>\n",
       "      <td>144.954147</td>\n",
       "      <td>-37.824257</td>\n",
       "      <td>Day two has kicked off #EduTECHAU!\\n\\nMeet our...</td>\n",
       "      <td>1468040114781655040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517273</th>\n",
       "      <td>1557448918329266176</td>\n",
       "      <td>2022-08-10</td>\n",
       "      <td>152.993195</td>\n",
       "      <td>-27.382143</td>\n",
       "      <td>@bluboy43 @TaikaWaititi People will still like...</td>\n",
       "      <td>20742804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517274</th>\n",
       "      <td>1557499571642593280</td>\n",
       "      <td>2022-08-10</td>\n",
       "      <td>150.931976</td>\n",
       "      <td>-33.848244</td>\n",
       "      <td>@AMCELL @puck_fair What’s happening here?</td>\n",
       "      <td>1010068200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517275</th>\n",
       "      <td>1557502623947030528</td>\n",
       "      <td>2022-08-10</td>\n",
       "      <td>150.931976</td>\n",
       "      <td>-33.848244</td>\n",
       "      <td>@AMCELL @puck_fair That’s sad.</td>\n",
       "      <td>1010068200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517276</th>\n",
       "      <td>1557495250264158208</td>\n",
       "      <td>2022-08-10</td>\n",
       "      <td>145.053131</td>\n",
       "      <td>-37.972565</td>\n",
       "      <td>@jhgates1 @Climatehope2 @S_D_Mannix @Mark_A_Lu...</td>\n",
       "      <td>1310276082</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517277 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         _id        date      x_cent     y_cent  \\\n",
       "0        1491567527322808321  2022-02-10  150.520142 -23.339474   \n",
       "1        1491693811663515654  2022-02-10  153.369354 -27.954222   \n",
       "2        1491674087378219009  2022-02-10  151.926880 -27.573589   \n",
       "3        1491721359587627008  2022-02-10  115.928314 -32.150101   \n",
       "4        1491595343838220291  2022-02-10  150.931976 -33.848244   \n",
       "...                      ...         ...         ...        ...   \n",
       "2517272  1557517139002925056  2022-08-10  144.954147 -37.824257   \n",
       "2517273  1557448918329266176  2022-08-10  152.993195 -27.382143   \n",
       "2517274  1557499571642593280  2022-08-10  150.931976 -33.848244   \n",
       "2517275  1557502623947030528  2022-08-10  150.931976 -33.848244   \n",
       "2517276  1557495250264158208  2022-08-10  145.053131 -37.972565   \n",
       "\n",
       "                                                      text  \\\n",
       "0        My life is hitting a big change soon. Keen to ...   \n",
       "1        @TobyRayEnglish @MikeDel21893959 @aSinister @b...   \n",
       "2                    @JadeArchaeobot @HarvardGSAS oh ewwww   \n",
       "3        @JadeArchaeobot @HarvardGSAS I'm so sorry you ...   \n",
       "4        @BehnamAkhavan @EngAustralia @Eng_IT_Sydney @S...   \n",
       "...                                                    ...   \n",
       "2517272  Day two has kicked off #EduTECHAU!\\n\\nMeet our...   \n",
       "2517273  @bluboy43 @TaikaWaititi People will still like...   \n",
       "2517274          @AMCELL @puck_fair What’s happening here?   \n",
       "2517275                     @AMCELL @puck_fair That’s sad.   \n",
       "2517276  @jhgates1 @Climatehope2 @S_D_Mannix @Mark_A_Lu...   \n",
       "\n",
       "                   author_id  is_political  \n",
       "0                   45472006             0  \n",
       "1         952342256823943168             0  \n",
       "2                   25033901             0  \n",
       "3                 3103790508             0  \n",
       "4        1249497357944672257             0  \n",
       "...                      ...           ...  \n",
       "2517272  1468040114781655040             0  \n",
       "2517273             20742804             0  \n",
       "2517274           1010068200             0  \n",
       "2517275           1010068200             0  \n",
       "2517276           1310276082             0  \n",
       "\n",
       "[2517277 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../data/curated/Twitter/twitter.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sentiment analysis\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "def get_sentiment(x):\n",
    "    \"\"\" Helper to extract sentiment \"\"\"\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sia_out = sia.polarity_scores(x)\n",
    "\n",
    "    neg = sia_out['neg']\n",
    "    pos = sia_out['pos']\n",
    "    compound = sia_out['compound']\n",
    "\n",
    "    return neg, pos, compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"neg_score\", \"pos_score\", \"compound_score\", \"sentiment\"]] = df[\"text\"].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "df.to_parquet('../data/curated/twitter_sentiment.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
