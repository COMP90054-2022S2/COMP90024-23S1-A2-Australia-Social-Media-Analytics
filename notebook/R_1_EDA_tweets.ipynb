{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R_1 Brief EDA of Twitter Data + NLTK sentiment analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open first 1000 lines and observe\n",
    "\n",
    "with open ('../data/mnt/ext100/twitter-huge.json', 'r') as f:\n",
    "    for i in range(1000):\n",
    "        line = f.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn just first line into a dictionary to observe contents\n",
    "dict = eval(line)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict:\n",
    "    print(key, dict[key])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the format of different content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1491571754170421255'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id\n",
      "_rev\n",
      "data\n",
      "matching_rules\n"
     ]
    }
   ],
   "source": [
    "for x in dict['doc']:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author_id\n",
      "conversation_id\n",
      "created_at\n",
      "entities\n",
      "geo\n",
      "lang\n",
      "public_metrics\n",
      "text\n",
      "sentiment\n"
     ]
    }
   ],
   "source": [
    "for x in dict['doc']['data']:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-02-10T00:36:32.000Z'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict['doc']['data']['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "There are no type variables left in dict['doc']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_64449/2646303918.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# can process whether match keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: There are no type variables left in dict['doc']"
     ]
    }
   ],
   "source": [
    "dict['doc']['data']['text'] # can process whether match keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008928571428571428"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict['doc']['data']['sentiment'] # have a look at the sentiment that it came with"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try NLTK Sentiment Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/tg.chenny/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/names.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tg.chenny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/tg.chenny/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/state_union.zip.\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/tg.chenny/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/tg.chenny/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tg.chenny/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/tg.chenny/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /Users/tg.chenny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download([\n",
    "        \"names\",\n",
    "        \"stopwords\",\n",
    "        \"state_union\",\n",
    "        \"twitter_samples\",\n",
    "        \"movie_reviews\",\n",
    "        \"averaged_perceptron_tagger\",\n",
    "        \"vader_lexicon\",\n",
    "        \"punkt\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.976, 'pos': 0.024, 'compound': 0.3527}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(dict['doc']['data']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.951, 'pos': 0.049, 'compound': 0.3527}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"If hypothetically an ancient 700 year's or so Jewish temple was found in another country, say Spain or ??, Zionist's would claim that country as an ancient sister homeland of Israel then probably find some ancient text in 1 of their vault's saying that God gave them that land too\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
