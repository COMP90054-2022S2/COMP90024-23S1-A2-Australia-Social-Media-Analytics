{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R_2 Through EDA of 65 GB tweets on the topic of Politics\n",
    "\n",
    "- to gauge how many political tweets there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/23 17:06:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"COMP90024_A2_EDA\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read In Original\n",
    "spark_json = spark.read.json('../data/raw/BigTwitterFile/twitter-huge.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- doc: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- _rev: string (nullable = true)\n",
      " |    |-- data: struct (nullable = true)\n",
      " |    |    |-- author_id: string (nullable = true)\n",
      " |    |    |-- context_annotations: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- domain: struct (nullable = true)\n",
      " |    |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- entity: struct (nullable = true)\n",
      " |    |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- conversation_id: string (nullable = true)\n",
      " |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |-- entities: struct (nullable = true)\n",
      " |    |    |    |-- annotations: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |    |-- normalized_text: string (nullable = true)\n",
      " |    |    |    |    |    |-- probability: double (nullable = true)\n",
      " |    |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- cashtags: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |    |-- tag: string (nullable = true)\n",
      " |    |    |    |-- hashtags: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |    |-- tag: string (nullable = true)\n",
      " |    |    |    |-- mentions: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |    |-- images: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- media_key: string (nullable = true)\n",
      " |    |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |    |-- status: long (nullable = true)\n",
      " |    |    |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |    |    |    |-- unwound_url: string (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- geo: struct (nullable = true)\n",
      " |    |    |    |-- coordinates: struct (nullable = true)\n",
      " |    |    |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- place_id: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- public_metrics: struct (nullable = true)\n",
      " |    |    |    |-- like_count: long (nullable = true)\n",
      " |    |    |    |-- quote_count: long (nullable = true)\n",
      " |    |    |    |-- reply_count: long (nullable = true)\n",
      " |    |    |    |-- retweet_count: long (nullable = true)\n",
      " |    |    |-- sentiment: double (nullable = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- withheld: struct (nullable = true)\n",
      " |    |    |    |-- copyright: boolean (nullable = true)\n",
      " |    |    |    |-- country_codes: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- includes: string (nullable = true)\n",
      " |    |-- matching_rules: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- tag: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- key: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- tags: string (nullable = true)\n",
      " |    |-- tokens: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observe Schema\n",
    "spark_json.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Key Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important features to extract out of the original dataframe\n",
    "out_df = spark_json.select('doc._id', 'doc.data.created_at', 'doc.data.geo.coordinates.coordinates', \n",
    "                  'doc.data.geo.coordinates.type', 'doc.includes', \n",
    "                  'doc.data.geo.place_id', 'doc.data.lang', 'doc.data.sentiment', 'doc.data.text', \n",
    "                  'doc.data.author_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49300230"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe missing values in 'includes' (geolocation)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "def count_missing_values(df, column_name):\n",
    "    # Filter the DataFrame to select rows where the column is null\n",
    "    filtered_df = df.filter(col(column_name).isNull())\n",
    "    \n",
    "    # Use the `sum()` function to count the number of rows with null values\n",
    "    count = filtered_df.select(sum(col(column_name).isNull().cast(\"int\"))).collect()[0][0]\n",
    "    \n",
    "    return count\n",
    "\n",
    "missing_count = count_missing_values(out_df, \"includes\")\n",
    "\n",
    "missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52533743"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3233513"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52533743 - 49300230"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Tweets containing AI Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "words_df = pd.read_excel('../data/raw/Keywords/Voting Keyword.xlsx')\n",
    "\n",
    "words = list(words_df['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only get tweets that contain AI related keywords\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assume 'df' is your DataFrame with a string typed column named 'text'\n",
    "filter_cond = col('doc.data.text').contains(words[0])  # initial filter condition\n",
    "\n",
    "for i in range(1, len(words)):\n",
    "    filter_cond = filter_cond | col('text').contains(words[i])  # add each snippet to the filter condition using the & operator\n",
    "\n",
    "filtered_df = out_df.filter(filter_cond)  # apply the filter condition to the DataFrame\n",
    "\n",
    "result_df = filtered_df.filter(col(\"doc.includes\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "319631"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert to Pandas df\n",
    "df = result_df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49952"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 'real AI related' tweets and has useable geolocation\n",
    "len(df[(~df['includes'].isnull())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observes some of these real political related tweet\n",
    "\n",
    "i = 0\n",
    "for txt in df[(~df['includes'].isnull())]['text']:\n",
    "    print(txt)\n",
    "    print('==')\n",
    "    i += 1\n",
    "\n",
    "    if i == 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"places\":[{\"full_name\":\"Brisbane, Queensland\",\"geo\":{\"type\":\"Feature\",\"bbox\":[152.668522848,-27.767440994,153.31787024,-26.996844991],\"properties\":{}},\"id\":\"004ec16c62325149\"}]}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA location\n",
    "df.loc[0]['includes']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER EDA (NOT IMPORTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4093"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many coordinates\n",
    "coord = filtered_df['coordinates']\n",
    "coord = [x for x in coord if x != None]\n",
    "len(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62422"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many place ids\n",
    "place_id = filtered_df['place_id']\n",
    "place_id = [x for x in place_id if x != None]\n",
    "\n",
    "len(place_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:======================================================>(469 + 2) / 471]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum datetime: 2022-02-10T00:00:00.000Z\n",
      "Maximum datetime: 2022-08-10T23:59:59.000Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check time range\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# assuming your data is stored in a Spark DataFrame called 'df'\n",
    "min_datetime = spark_json.agg(min(\"doc.data.created_at\")).collect()[0][0]\n",
    "max_datetime = spark_json.agg(max(\"doc.data.created_at\")).collect()[0][0]\n",
    "\n",
    "print(\"Minimum datetime:\", min_datetime)\n",
    "print(\"Maximum datetime:\", max_datetime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
